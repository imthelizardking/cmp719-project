{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOKuh3ypDqv9nNfgTxmfGkd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imthelizardking/cmp719-project/blob/main/cmp719_project_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main text"
      ],
      "metadata": {
        "id": "-ceqk4KZuraO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount gdrive for saving weights etc."
      ],
      "metadata": {
        "id": "q1NDzwFXuszW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!cd '/content/drive/MyDrive'"
      ],
      "metadata": {
        "id": "9-vEFh6prUQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b1adbf-bf5a-4e86-eb04-06613ff11afa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required packages"
      ],
      "metadata": {
        "id": "6hRERpdouwyw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ua7XRff-OG-R",
        "outputId": "56a8297e-c3fb-4172-cc1c-d15a60bd5a81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "#import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet-56 Model:"
      ],
      "metadata": {
        "id": "fp0wY1YCs9IO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ResNet-56 model\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(x)  # Skip connection\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet56(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNet56, self).__init__()\n",
        "        self.in_planes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(16, 9, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 9, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 9, stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = nn.ReLU()(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = nn.AdaptiveAvgPool2d(1)(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "vQoHFy4iPJIn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set configuration for training ResNet-56 w/ cifar-100:"
      ],
      "metadata": {
        "id": "A3a-YsRwu0_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg55HD5COLmp",
        "outputId": "8de7a791-dab7-44df-a660-261a2a1a95c7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(trainloader))"
      ],
      "metadata": {
        "id": "OdbUk_MkoqmZ",
        "outputId": "a733960e-3870-4537-92e0-fa12f6543739",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ResNet-56 model instance\n",
        "model_resnet56 = ResNet56(num_classes=100).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "EPOCHS_RESNET56 = 10\n",
        "LEARNING_RATE_RESNET56 = 0.001\n",
        "optimizer = optim.AdamW(model_resnet56.parameters(), lr=LEARNING_RATE_RESNET56)"
      ],
      "metadata": {
        "id": "5tGvSvJBvMyS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train ResNet-56 w/ cifar-100"
      ],
      "metadata": {
        "id": "0ScoZdfZu4F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "for epoch in range(EPOCHS_RESNET56):  # Number of epochs\n",
        "    model_resnet56.train()\n",
        "    train_loss = 0\n",
        "    train_accuracy = 0\n",
        "    test_loss = 0\n",
        "    test_accuracy = 0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model_resnet56(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, train_loss / len(trainloader)))\n",
        "    train_losses.append(train_loss / len(trainloader))\n",
        "    # Validation\n",
        "    model_resnet56.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    torch._to_functional_tensor = 0\n",
        "    with torch.no_grad():\n",
        "        for data in trainloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model_resnet56(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    train_accuracy = correct / total\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model_resnet56(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    test_loss = test_loss / len(testloader)\n",
        "    test_losses.append(test_loss / len(testloader))\n",
        "    test_accuracy = 100 * correct / total\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    print('Accuracy on test set after epoch %d: %.2f %%' % (epoch + 1, test_accuracy))\n",
        "print('Training finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "sIMt9YZWqFX7",
        "outputId": "c4a23cad-08e4-43bb-9eb3-8a5bcdb99621"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-dbaf3ee07af7>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save trained ResNet-56 weights:"
      ],
      "metadata": {
        "id": "vClLBAJtuUmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_resnet56.state_dict(), '/content/drive/MyDrive/719_project/trained_weights/resnet_model_weights.pth')"
      ],
      "metadata": {
        "id": "-gahXtc9dc0l",
        "outputId": "6e42b597-42da-494b-d903-9bdbfd9b6b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9dbe6edb2882>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_resnet56\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/719_project/trained_weights/resnet_model_weights.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Top-1 accuracy for trained ResNet-56 model and cifar-100 dataset (in paper, 70.43%):"
      ],
      "metadata": {
        "id": "fjgi4ZDmuXFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # for making sure there is no training, just inference\n",
        "    model_resnet56.eval()  # model to eval. mode\n",
        "    for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model_resnet56(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "top1_accuracy = 100 * correct / total\n",
        "print('Top-1 Accuracy: {:.2f}%'.format(top1_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xra3b6yxsrkU",
        "outputId": "f1f8a1f4-ee56-4462-8f09-b89406757ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 Accuracy: 41.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vision Transformer w/ Feature Guidance:**"
      ],
      "metadata": {
        "id": "x_f-1YhVwFmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vision Transformer w/ feature guidance:"
      ],
      "metadata": {
        "id": "Lyc2MRLQwzO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class T2TViT(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, num_classes, embed_dim, depth, heads, mlp_dim, token_dim):\n",
        "        super(T2TViT, self).__init__()\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "        self.depth = depth\n",
        "        self.heads = heads\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.num_tokens = self.num_patches\n",
        "        self.token_dim = token_dim\n",
        "\n",
        "        self.patch_embeddings = nn.Conv2d(in_channels, self.token_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.token_embeddings = nn.Parameter(torch.randn(1, self.num_tokens, self.token_dim))\n",
        "\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(embed_dim, heads, mlp_dim),\n",
        "            depth\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch embeddings\n",
        "        x = self.patch_embeddings(x)\n",
        "\n",
        "        # Reshaping the patches\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "        # Token embeddings\n",
        "        tokens = self.token_embeddings.repeat(x.shape[0], 1, 1)\n",
        "\n",
        "        # Concatenate token embeddings with patch embeddings\n",
        "        x = torch.cat((tokens, x), dim=1)  # Concatenate along the second dimension\n",
        "\n",
        "        # Transformer layers\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Global pooling (mean)\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        # Classification\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "oLqg-Vm8wGOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Vision Transformer model\n",
        "class VisionTransformer_fg(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim):\n",
        "        super(VisionTransformer_fg, self).__init__()\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = 3 * patch_size ** 2  # Assuming RGB images\n",
        "\n",
        "        self.patch_embedding = nn.Sequential(\n",
        "            nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),\n",
        "            nn.Flatten(start_dim=2)\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(dim, heads, dim_feedforward=mlp_dim),\n",
        "            num_layers=depth\n",
        "        )\n",
        "        self.classifier = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=0)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YX26rgWHwHo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set training configuration for Vision Transformer w/ feature guidance"
      ],
      "metadata": {
        "id": "QecnTTJPwges"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up hyperparameters and data loaders\n",
        "torch.cuda.empty_cache()\n",
        "image_size = 32\n",
        "patch_size = 16\n",
        "num_classes = 100\n",
        "dim = 128 #64\n",
        "depth = 7 #10\n",
        "heads = 4\n",
        "token_dim = 128\n",
        "mlp_dim = 256 #512\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "epochs = 50\n",
        "BETA = 2.5 # scaler for feature guidance loss\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "\n",
        "# Create an instance of the model\n",
        "#model_ViT = VisionTransformer_fg(image_size, patch_size, num_classes, dim, depth, heads, mlp_dim)\n",
        "model_ViT = T2TViT(image_size, patch_size, 3, num_classes, dim, depth, heads, mlp_dim, token_dim)\n",
        "\n",
        "model_ViT.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_ViT.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "# Create data loaders (replace with your own datasets)\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qPmWYBzwl6x",
        "outputId": "0234145a-9626-4d35-dbbd-7fdb7a4d7521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If pre-trained weights will be used, run following code snippet:"
      ],
      "metadata": {
        "id": "f3mUQRs2y0V-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ViT.load_state_dict(torch.load('/content/drive/MyDrive/719_project/trained_weights/ViT_model_weights.pth'))"
      ],
      "metadata": {
        "id": "HfN4ByVJy4d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vision Transformer Trainer:"
      ],
      "metadata": {
        "id": "q5tBSOANwJum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "#model_resnet56.eval() # no training for cnn, just eval.\n",
        "model_ViT.train()\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    for images, labels in train_dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # zero grads\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model_ViT(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_train_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o86la3xZ-SZS",
        "outputId": "0f89edd9-52cc-489f-dc14-fc1c843bf8df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 - Loss: 4.3495\n",
            "Epoch 2/50 - Loss: 4.2698\n",
            "Epoch 3/50 - Loss: 4.2106\n",
            "Epoch 4/50 - Loss: 4.1531\n",
            "Epoch 5/50 - Loss: 4.1188\n",
            "Epoch 6/50 - Loss: 4.0963\n",
            "Epoch 7/50 - Loss: 4.0849\n",
            "Epoch 8/50 - Loss: 4.0796\n",
            "Epoch 9/50 - Loss: 4.0713\n",
            "Epoch 10/50 - Loss: 4.0649\n",
            "Epoch 11/50 - Loss: 4.0593\n",
            "Epoch 12/50 - Loss: 4.0578\n",
            "Epoch 13/50 - Loss: 4.0503\n",
            "Epoch 14/50 - Loss: 4.0532\n",
            "Epoch 15/50 - Loss: 4.0450\n",
            "Epoch 16/50 - Loss: 4.0472\n",
            "Epoch 17/50 - Loss: 4.0429\n",
            "Epoch 18/50 - Loss: 4.0441\n",
            "Epoch 19/50 - Loss: 4.0366\n",
            "Epoch 20/50 - Loss: 4.0358\n",
            "Epoch 21/50 - Loss: 4.0352\n",
            "Epoch 22/50 - Loss: 4.0337\n",
            "Epoch 23/50 - Loss: 4.0305\n",
            "Epoch 24/50 - Loss: 4.0273\n",
            "Epoch 25/50 - Loss: 4.0277\n",
            "Epoch 26/50 - Loss: 4.0224\n",
            "Epoch 27/50 - Loss: 4.0266\n",
            "Epoch 28/50 - Loss: 4.0211\n",
            "Epoch 29/50 - Loss: 4.0210\n",
            "Epoch 30/50 - Loss: 4.0200\n",
            "Epoch 31/50 - Loss: 4.0184\n",
            "Epoch 32/50 - Loss: 4.0156\n",
            "Epoch 33/50 - Loss: 4.0162\n",
            "Epoch 34/50 - Loss: 4.0186\n",
            "Epoch 35/50 - Loss: 4.0146\n",
            "Epoch 36/50 - Loss: 4.0120\n",
            "Epoch 37/50 - Loss: 4.0116\n",
            "Epoch 38/50 - Loss: 4.0060\n",
            "Epoch 39/50 - Loss: 4.0086\n",
            "Epoch 40/50 - Loss: 4.0080\n",
            "Epoch 41/50 - Loss: 4.0021\n",
            "Epoch 42/50 - Loss: 4.0051\n",
            "Epoch 43/50 - Loss: 4.0007\n",
            "Epoch 44/50 - Loss: 4.0012\n",
            "Epoch 45/50 - Loss: 4.0005\n",
            "Epoch 46/50 - Loss: 4.0008\n",
            "Epoch 47/50 - Loss: 4.0006\n",
            "Epoch 48/50 - Loss: 3.9984\n",
            "Epoch 49/50 - Loss: 4.0011\n",
            "Epoch 50/50 - Loss: 3.9978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Top-1 Accuracy for Vision Transformer w/ feature guidance:"
      ],
      "metadata": {
        "id": "Vs5z4OQCKIU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_ViT.state_dict(), '/content/drive/MyDrive/719_project/trained_weights/ViT_model_weights.pth')"
      ],
      "metadata": {
        "id": "At_C5tkjrna9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # for making sure there is no training, just inference\n",
        "    model_ViT.eval()  # model to eval. mode\n",
        "    total, correct = 0, 0\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model_ViT(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "top1_accuracy = 100 * correct / total\n",
        "print('Top-1 Accuracy: {:.2f}%'.format(top1_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K79DxOQaKChG",
        "outputId": "b7bfa789-3dfa-4287-82a8-c1685d97e203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 Accuracy: 7.43%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEMP:"
      ],
      "metadata": {
        "id": "f00AkMq0_mNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training loop\n",
        "def train_ViT_with_fg(model_ViT, model_cnn, dataloader, criterion, optimizer, device, BETA):\n",
        "\n",
        "    ###def get_features_hook(module, input, output):\n",
        "      #### Store the intermediate features in a global variable\n",
        "      ###global student_features\n",
        "      ###student_features = output\n",
        "    ###def get_teacher_features_hook(module, input, output):\n",
        "        #### Store the intermediate features in a global variable\n",
        "        ###global teacher_features\n",
        "        ###teacher_features = output\n",
        "\n",
        "    model_cnn.eval() # no training for cnn, just eval.\n",
        "    model_ViT.train()\n",
        "    total_loss = 0.0\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        #images, labels = images.cuda(), labels.cuda() # add this line\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_ViT(images)\n",
        "\n",
        "        #### hook cnn and ViT for intermediate feature extraction #\n",
        "        ###criterion_fg = nn.MSELoss()\n",
        "        ###model_ViT.register_forward_hook(get_features_hook)\n",
        "        #### Register a forward hook to extract features from the teacher model\n",
        "        ###model_cnn.register_forward_hook(get_teacher_features_hook)\n",
        "        ####loss_fg = criterion_fg(student_features, teacher_features.detach())  # detach the teacher features to prevent backpropagation through the teacher\n",
        "        ###loss_fg = 0\n",
        "        # hook cnn and ViT for intermediate feature extraction #\n",
        "        loss_cls = criterion(outputs, labels) # cross-entropy loss\n",
        "        loss = loss_cls + BETA * 0 # loss_fg\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "    return total_loss / len(dataloader.dataset)"
      ],
      "metadata": {
        "id": "OYTQvwCu_puJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}