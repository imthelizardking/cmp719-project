{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMelcFmqleWBJGSMehLhKzS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imthelizardking/cmp719-project/blob/main/cmp719_project_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main text"
      ],
      "metadata": {
        "id": "-ceqk4KZuraO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount gdrive for saving weights etc."
      ],
      "metadata": {
        "id": "q1NDzwFXuszW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!cd '/content/drive/MyDrive'"
      ],
      "metadata": {
        "id": "9-vEFh6prUQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac178483-8680-4279-a1f6-9dd0b9405b67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required packages"
      ],
      "metadata": {
        "id": "6hRERpdouwyw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ua7XRff-OG-R",
        "outputId": "fe1867dd-f2fb-4c22-bf7d-e897da75d8fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet-56 Model:"
      ],
      "metadata": {
        "id": "fp0wY1YCs9IO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ResNet-56 model\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Adjust the number of input channels for the skip connection\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(x)  # Skip connection\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet56(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNet56, self).__init__()\n",
        "        self.in_planes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(16, 9, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 9, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 9, stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * BasicBlock.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = nn.ReLU()(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = nn.AdaptiveAvgPool2d(1)(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "vQoHFy4iPJIn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set configuration for training ResNet-56 w/ cifar-100:"
      ],
      "metadata": {
        "id": "A3a-YsRwu0_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg55HD5COLmp",
        "outputId": "e89fcf61-3cf7-4bdc-bb88-08ad5d15840f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ResNet-56 model instance\n",
        "model_resnet56 = ResNet56(num_classes=100).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model_resnet56.parameters(), lr=0.001)\n",
        "EPOCHS_RESNET56 = 300"
      ],
      "metadata": {
        "id": "5tGvSvJBvMyS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train ResNet-56 w/ cifar-100"
      ],
      "metadata": {
        "id": "0ScoZdfZu4F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(EPOCHS_RESNET56):  # Number of epochs\n",
        "    model_resnet56.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_resnet56(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 100 == 99:  # Print every 100 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Validation\n",
        "    model_resnet56.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model_resnet56(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print('Accuracy on test set after epoch %d: %.2f %%' % (epoch + 1, accuracy))\n",
        "\n",
        "print('Training finished.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIMt9YZWqFX7",
        "outputId": "f1943f07-22e9-4a84-bf99-04c24f1d8766"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 4.442\n",
            "[1,   200] loss: 4.092\n",
            "[1,   300] loss: 3.925\n",
            "Accuracy on test set after epoch 1: 9.82 %\n",
            "[2,   100] loss: 3.683\n",
            "[2,   200] loss: 3.569\n",
            "[2,   300] loss: 3.483\n",
            "Accuracy on test set after epoch 2: 18.80 %\n",
            "[3,   100] loss: 3.296\n",
            "[3,   200] loss: 3.209\n",
            "[3,   300] loss: 3.099\n",
            "Accuracy on test set after epoch 3: 22.28 %\n",
            "[4,   100] loss: 2.923\n",
            "[4,   200] loss: 2.842\n",
            "[4,   300] loss: 2.767\n",
            "Accuracy on test set after epoch 4: 29.28 %\n",
            "[5,   100] loss: 2.629\n",
            "[5,   200] loss: 2.577\n",
            "[5,   300] loss: 2.511\n",
            "Accuracy on test set after epoch 5: 32.79 %\n",
            "[6,   100] loss: 2.365\n",
            "[6,   200] loss: 2.362\n",
            "[6,   300] loss: 2.324\n",
            "Accuracy on test set after epoch 6: 35.71 %\n",
            "[7,   100] loss: 2.237\n",
            "[7,   200] loss: 2.188\n",
            "[7,   300] loss: 2.135\n",
            "Accuracy on test set after epoch 7: 40.29 %\n",
            "[8,   100] loss: 2.069\n",
            "[8,   200] loss: 2.055\n",
            "[8,   300] loss: 2.031\n",
            "Accuracy on test set after epoch 8: 42.33 %\n",
            "[9,   100] loss: 1.937\n",
            "[9,   200] loss: 1.911\n",
            "[9,   300] loss: 1.898\n",
            "Accuracy on test set after epoch 9: 43.68 %\n",
            "[10,   100] loss: 1.811\n",
            "[10,   200] loss: 1.810\n",
            "[10,   300] loss: 1.830\n",
            "Accuracy on test set after epoch 10: 47.39 %\n",
            "[11,   100] loss: 1.730\n",
            "[11,   200] loss: 1.715\n",
            "[11,   300] loss: 1.712\n",
            "Accuracy on test set after epoch 11: 48.64 %\n",
            "[12,   100] loss: 1.650\n",
            "[12,   200] loss: 1.630\n",
            "[12,   300] loss: 1.671\n",
            "Accuracy on test set after epoch 12: 48.40 %\n",
            "[13,   100] loss: 1.578\n",
            "[13,   200] loss: 1.613\n",
            "[13,   300] loss: 1.587\n",
            "Accuracy on test set after epoch 13: 50.89 %\n",
            "[14,   100] loss: 1.492\n",
            "[14,   200] loss: 1.537\n",
            "[14,   300] loss: 1.519\n",
            "Accuracy on test set after epoch 14: 51.89 %\n",
            "[15,   100] loss: 1.450\n",
            "[15,   200] loss: 1.483\n",
            "[15,   300] loss: 1.485\n",
            "Accuracy on test set after epoch 15: 51.84 %\n",
            "[16,   100] loss: 1.409\n",
            "[16,   200] loss: 1.409\n",
            "[16,   300] loss: 1.426\n",
            "Accuracy on test set after epoch 16: 53.42 %\n",
            "[17,   100] loss: 1.340\n",
            "[17,   200] loss: 1.380\n",
            "[17,   300] loss: 1.371\n",
            "Accuracy on test set after epoch 17: 53.46 %\n",
            "[18,   100] loss: 1.282\n",
            "[18,   200] loss: 1.335\n",
            "[18,   300] loss: 1.347\n",
            "Accuracy on test set after epoch 18: 55.29 %\n",
            "[19,   100] loss: 1.274\n",
            "[19,   200] loss: 1.240\n",
            "[19,   300] loss: 1.308\n",
            "Accuracy on test set after epoch 19: 55.72 %\n",
            "[20,   100] loss: 1.226\n",
            "[20,   200] loss: 1.218\n",
            "[20,   300] loss: 1.246\n",
            "Accuracy on test set after epoch 20: 56.64 %\n",
            "[21,   100] loss: 1.181\n",
            "[21,   200] loss: 1.193\n",
            "[21,   300] loss: 1.222\n",
            "Accuracy on test set after epoch 21: 56.54 %\n",
            "[22,   100] loss: 1.155\n",
            "[22,   200] loss: 1.203\n",
            "[22,   300] loss: 1.174\n",
            "Accuracy on test set after epoch 22: 57.97 %\n",
            "[23,   100] loss: 1.092\n",
            "[23,   200] loss: 1.151\n",
            "[23,   300] loss: 1.151\n",
            "Accuracy on test set after epoch 23: 56.41 %\n",
            "[24,   100] loss: 1.078\n",
            "[24,   200] loss: 1.090\n",
            "[24,   300] loss: 1.136\n",
            "Accuracy on test set after epoch 24: 58.10 %\n",
            "[25,   100] loss: 1.044\n",
            "[25,   200] loss: 1.065\n",
            "[25,   300] loss: 1.103\n",
            "Accuracy on test set after epoch 25: 58.06 %\n",
            "[26,   100] loss: 1.021\n",
            "[26,   200] loss: 1.049\n",
            "[26,   300] loss: 1.069\n",
            "Accuracy on test set after epoch 26: 57.32 %\n",
            "[27,   100] loss: 0.990\n",
            "[27,   200] loss: 1.030\n",
            "[27,   300] loss: 1.022\n",
            "Accuracy on test set after epoch 27: 57.36 %\n",
            "[28,   100] loss: 0.976\n",
            "[28,   200] loss: 0.998\n",
            "[28,   300] loss: 1.015\n",
            "Accuracy on test set after epoch 28: 58.86 %\n",
            "[29,   100] loss: 0.944\n",
            "[29,   200] loss: 0.975\n",
            "[29,   300] loss: 1.000\n",
            "Accuracy on test set after epoch 29: 58.77 %\n",
            "[30,   100] loss: 0.913\n",
            "[30,   200] loss: 0.943\n",
            "[30,   300] loss: 0.965\n",
            "Accuracy on test set after epoch 30: 59.56 %\n",
            "[31,   100] loss: 0.899\n",
            "[31,   200] loss: 0.926\n",
            "[31,   300] loss: 0.944\n",
            "Accuracy on test set after epoch 31: 59.11 %\n",
            "[32,   100] loss: 0.882\n",
            "[32,   200] loss: 0.896\n",
            "[32,   300] loss: 0.907\n",
            "Accuracy on test set after epoch 32: 60.29 %\n",
            "[33,   100] loss: 0.863\n",
            "[33,   200] loss: 0.854\n",
            "[33,   300] loss: 0.898\n",
            "Accuracy on test set after epoch 33: 60.08 %\n",
            "[34,   100] loss: 0.825\n",
            "[34,   200] loss: 0.889\n",
            "[34,   300] loss: 0.885\n",
            "Accuracy on test set after epoch 34: 59.93 %\n",
            "[35,   100] loss: 0.822\n",
            "[35,   200] loss: 0.840\n",
            "[35,   300] loss: 0.883\n",
            "Accuracy on test set after epoch 35: 60.13 %\n",
            "[36,   100] loss: 0.779\n",
            "[36,   200] loss: 0.834\n",
            "[36,   300] loss: 0.855\n",
            "Accuracy on test set after epoch 36: 61.20 %\n",
            "[37,   100] loss: 0.786\n",
            "[37,   200] loss: 0.803\n",
            "[37,   300] loss: 0.836\n",
            "Accuracy on test set after epoch 37: 59.33 %\n",
            "[38,   100] loss: 0.763\n",
            "[38,   200] loss: 0.777\n",
            "[38,   300] loss: 0.794\n",
            "Accuracy on test set after epoch 38: 59.68 %\n",
            "[39,   100] loss: 0.742\n",
            "[39,   200] loss: 0.767\n",
            "[39,   300] loss: 0.779\n",
            "Accuracy on test set after epoch 39: 61.09 %\n",
            "[40,   100] loss: 0.716\n",
            "[40,   200] loss: 0.754\n",
            "[40,   300] loss: 0.767\n",
            "Accuracy on test set after epoch 40: 60.19 %\n",
            "[41,   100] loss: 0.695\n",
            "[41,   200] loss: 0.763\n",
            "[41,   300] loss: 0.760\n",
            "Accuracy on test set after epoch 41: 60.52 %\n",
            "[42,   100] loss: 0.700\n",
            "[42,   200] loss: 0.709\n",
            "[42,   300] loss: 0.735\n",
            "Accuracy on test set after epoch 42: 60.83 %\n",
            "[43,   100] loss: 0.664\n",
            "[43,   200] loss: 0.703\n",
            "[43,   300] loss: 0.747\n",
            "Accuracy on test set after epoch 43: 59.49 %\n",
            "[44,   100] loss: 0.657\n",
            "[44,   200] loss: 0.711\n",
            "[44,   300] loss: 0.701\n",
            "Accuracy on test set after epoch 44: 60.56 %\n",
            "[45,   100] loss: 0.646\n",
            "[45,   200] loss: 0.672\n",
            "[45,   300] loss: 0.690\n",
            "Accuracy on test set after epoch 45: 60.24 %\n",
            "[46,   100] loss: 0.629\n",
            "[46,   200] loss: 0.669\n",
            "[46,   300] loss: 0.698\n",
            "Accuracy on test set after epoch 46: 62.31 %\n",
            "[47,   100] loss: 0.613\n",
            "[47,   200] loss: 0.654\n",
            "[47,   300] loss: 0.680\n",
            "Accuracy on test set after epoch 47: 61.54 %\n",
            "[48,   100] loss: 0.615\n",
            "[48,   200] loss: 0.649\n",
            "[48,   300] loss: 0.674\n",
            "Accuracy on test set after epoch 48: 60.85 %\n",
            "[49,   100] loss: 0.613\n",
            "[49,   200] loss: 0.638\n",
            "[49,   300] loss: 0.666\n",
            "Accuracy on test set after epoch 49: 62.14 %\n",
            "[50,   100] loss: 0.602\n",
            "[50,   200] loss: 0.616\n",
            "[50,   300] loss: 0.640\n",
            "Accuracy on test set after epoch 50: 60.28 %\n",
            "[51,   100] loss: 0.594\n",
            "[51,   200] loss: 0.611\n",
            "[51,   300] loss: 0.614\n",
            "Accuracy on test set after epoch 51: 61.27 %\n",
            "[52,   100] loss: 0.582\n",
            "[52,   200] loss: 0.592\n",
            "[52,   300] loss: 0.615\n",
            "Accuracy on test set after epoch 52: 61.36 %\n",
            "[53,   100] loss: 0.549\n",
            "[53,   200] loss: 0.594\n",
            "[53,   300] loss: 0.621\n",
            "Accuracy on test set after epoch 53: 60.51 %\n",
            "[54,   100] loss: 0.544\n",
            "[54,   200] loss: 0.566\n",
            "[54,   300] loss: 0.585\n",
            "Accuracy on test set after epoch 54: 61.21 %\n",
            "[55,   100] loss: 0.543\n",
            "[55,   200] loss: 0.592\n",
            "[55,   300] loss: 0.581\n",
            "Accuracy on test set after epoch 55: 60.98 %\n",
            "[56,   100] loss: 0.521\n",
            "[56,   200] loss: 0.557\n",
            "[56,   300] loss: 0.590\n",
            "Accuracy on test set after epoch 56: 61.78 %\n",
            "[57,   100] loss: 0.520\n",
            "[57,   200] loss: 0.531\n",
            "[57,   300] loss: 0.575\n",
            "Accuracy on test set after epoch 57: 61.93 %\n",
            "[58,   100] loss: 0.506\n",
            "[58,   200] loss: 0.542\n",
            "[58,   300] loss: 0.554\n",
            "Accuracy on test set after epoch 58: 61.91 %\n",
            "[59,   100] loss: 0.504\n",
            "[59,   200] loss: 0.530\n",
            "[59,   300] loss: 0.526\n",
            "Accuracy on test set after epoch 59: 62.05 %\n",
            "[60,   100] loss: 0.490\n",
            "[60,   200] loss: 0.535\n",
            "[60,   300] loss: 0.524\n",
            "Accuracy on test set after epoch 60: 61.22 %\n",
            "[61,   100] loss: 0.480\n",
            "[61,   200] loss: 0.519\n",
            "[61,   300] loss: 0.542\n",
            "Accuracy on test set after epoch 61: 61.47 %\n",
            "[62,   100] loss: 0.469\n",
            "[62,   200] loss: 0.517\n",
            "[62,   300] loss: 0.517\n",
            "Accuracy on test set after epoch 62: 62.06 %\n",
            "[63,   100] loss: 0.477\n",
            "[63,   200] loss: 0.480\n",
            "[63,   300] loss: 0.513\n",
            "Accuracy on test set after epoch 63: 62.33 %\n",
            "[64,   100] loss: 0.453\n",
            "[64,   200] loss: 0.483\n",
            "[64,   300] loss: 0.498\n",
            "Accuracy on test set after epoch 64: 61.57 %\n",
            "[65,   100] loss: 0.445\n",
            "[65,   200] loss: 0.497\n",
            "[65,   300] loss: 0.515\n",
            "Accuracy on test set after epoch 65: 62.10 %\n",
            "[66,   100] loss: 0.451\n",
            "[66,   200] loss: 0.485\n",
            "[66,   300] loss: 0.500\n",
            "Accuracy on test set after epoch 66: 62.03 %\n",
            "[67,   100] loss: 0.435\n",
            "[67,   200] loss: 0.460\n",
            "[67,   300] loss: 0.492\n",
            "Accuracy on test set after epoch 67: 61.80 %\n",
            "[68,   100] loss: 0.433\n",
            "[68,   200] loss: 0.459\n",
            "[68,   300] loss: 0.500\n",
            "Accuracy on test set after epoch 68: 62.89 %\n",
            "[69,   100] loss: 0.442\n",
            "[69,   200] loss: 0.460\n",
            "[69,   300] loss: 0.480\n",
            "Accuracy on test set after epoch 69: 62.82 %\n",
            "[70,   100] loss: 0.420\n",
            "[70,   200] loss: 0.445\n",
            "[70,   300] loss: 0.461\n",
            "Accuracy on test set after epoch 70: 60.92 %\n",
            "[71,   100] loss: 0.412\n",
            "[71,   200] loss: 0.461\n",
            "[71,   300] loss: 0.468\n",
            "Accuracy on test set after epoch 71: 62.52 %\n",
            "[72,   100] loss: 0.418\n",
            "[72,   200] loss: 0.429\n",
            "[72,   300] loss: 0.443\n",
            "Accuracy on test set after epoch 72: 61.58 %\n",
            "[73,   100] loss: 0.412\n",
            "[73,   200] loss: 0.423\n",
            "[73,   300] loss: 0.441\n",
            "Accuracy on test set after epoch 73: 61.62 %\n",
            "[74,   100] loss: 0.383\n",
            "[74,   200] loss: 0.418\n",
            "[74,   300] loss: 0.433\n",
            "Accuracy on test set after epoch 74: 62.63 %\n",
            "[75,   100] loss: 0.400\n",
            "[75,   200] loss: 0.402\n",
            "[75,   300] loss: 0.433\n",
            "Accuracy on test set after epoch 75: 62.20 %\n",
            "[76,   100] loss: 0.394\n",
            "[76,   200] loss: 0.406\n",
            "[76,   300] loss: 0.434\n",
            "Accuracy on test set after epoch 76: 62.64 %\n",
            "[77,   100] loss: 0.402\n",
            "[77,   200] loss: 0.396\n",
            "[77,   300] loss: 0.419\n",
            "Accuracy on test set after epoch 77: 63.02 %\n",
            "[78,   100] loss: 0.385\n",
            "[78,   200] loss: 0.393\n",
            "[78,   300] loss: 0.423\n",
            "Accuracy on test set after epoch 78: 61.63 %\n",
            "[79,   100] loss: 0.374\n",
            "[79,   200] loss: 0.401\n",
            "[79,   300] loss: 0.399\n",
            "Accuracy on test set after epoch 79: 62.56 %\n",
            "[80,   100] loss: 0.371\n",
            "[80,   200] loss: 0.389\n",
            "[80,   300] loss: 0.404\n",
            "Accuracy on test set after epoch 80: 61.94 %\n",
            "[81,   100] loss: 0.354\n",
            "[81,   200] loss: 0.378\n",
            "[81,   300] loss: 0.399\n",
            "Accuracy on test set after epoch 81: 62.08 %\n",
            "[82,   100] loss: 0.359\n",
            "[82,   200] loss: 0.359\n",
            "[82,   300] loss: 0.417\n",
            "Accuracy on test set after epoch 82: 62.29 %\n",
            "[83,   100] loss: 0.352\n",
            "[83,   200] loss: 0.372\n",
            "[83,   300] loss: 0.416\n",
            "Accuracy on test set after epoch 83: 63.05 %\n",
            "[84,   100] loss: 0.353\n",
            "[84,   200] loss: 0.353\n",
            "[84,   300] loss: 0.393\n",
            "Accuracy on test set after epoch 84: 61.93 %\n",
            "[85,   100] loss: 0.346\n",
            "[85,   200] loss: 0.356\n",
            "[85,   300] loss: 0.392\n",
            "Accuracy on test set after epoch 85: 62.23 %\n",
            "[86,   100] loss: 0.339\n",
            "[86,   200] loss: 0.363\n",
            "[86,   300] loss: 0.390\n",
            "Accuracy on test set after epoch 86: 62.97 %\n",
            "[87,   100] loss: 0.346\n",
            "[87,   200] loss: 0.354\n",
            "[87,   300] loss: 0.382\n",
            "Accuracy on test set after epoch 87: 63.05 %\n",
            "[88,   100] loss: 0.342\n",
            "[88,   200] loss: 0.336\n",
            "[88,   300] loss: 0.352\n",
            "Accuracy on test set after epoch 88: 61.88 %\n",
            "[89,   100] loss: 0.332\n",
            "[89,   200] loss: 0.351\n",
            "[89,   300] loss: 0.349\n",
            "Accuracy on test set after epoch 89: 63.26 %\n",
            "[90,   100] loss: 0.325\n",
            "[90,   200] loss: 0.342\n",
            "[90,   300] loss: 0.363\n",
            "Accuracy on test set after epoch 90: 62.85 %\n",
            "[91,   100] loss: 0.323\n",
            "[91,   200] loss: 0.348\n",
            "[91,   300] loss: 0.379\n",
            "Accuracy on test set after epoch 91: 62.97 %\n",
            "[92,   100] loss: 0.304\n",
            "[92,   200] loss: 0.336\n",
            "[92,   300] loss: 0.372\n",
            "Accuracy on test set after epoch 92: 63.03 %\n",
            "[93,   100] loss: 0.316\n",
            "[93,   200] loss: 0.319\n",
            "[93,   300] loss: 0.352\n",
            "Accuracy on test set after epoch 93: 62.24 %\n",
            "[94,   100] loss: 0.305\n",
            "[94,   200] loss: 0.336\n",
            "[94,   300] loss: 0.364\n",
            "Accuracy on test set after epoch 94: 61.30 %\n",
            "[95,   100] loss: 0.317\n",
            "[95,   200] loss: 0.318\n",
            "[95,   300] loss: 0.331\n",
            "Accuracy on test set after epoch 95: 62.89 %\n",
            "[96,   100] loss: 0.305\n",
            "[96,   200] loss: 0.309\n",
            "[96,   300] loss: 0.340\n",
            "Accuracy on test set after epoch 96: 63.29 %\n",
            "[97,   100] loss: 0.302\n",
            "[97,   200] loss: 0.313\n",
            "[97,   300] loss: 0.340\n",
            "Accuracy on test set after epoch 97: 62.40 %\n",
            "[98,   100] loss: 0.310\n",
            "[98,   200] loss: 0.322\n",
            "[98,   300] loss: 0.320\n",
            "Accuracy on test set after epoch 98: 62.32 %\n",
            "[99,   100] loss: 0.292\n",
            "[99,   200] loss: 0.315\n",
            "[99,   300] loss: 0.332\n",
            "Accuracy on test set after epoch 99: 62.76 %\n",
            "[100,   100] loss: 0.303\n",
            "[100,   200] loss: 0.320\n",
            "[100,   300] loss: 0.332\n",
            "Accuracy on test set after epoch 100: 62.54 %\n",
            "[101,   100] loss: 0.307\n",
            "[101,   200] loss: 0.323\n",
            "[101,   300] loss: 0.337\n",
            "Accuracy on test set after epoch 101: 62.54 %\n",
            "[102,   100] loss: 0.276\n",
            "[102,   200] loss: 0.304\n",
            "[102,   300] loss: 0.315\n",
            "Accuracy on test set after epoch 102: 62.37 %\n",
            "[103,   100] loss: 0.284\n",
            "[103,   200] loss: 0.291\n",
            "[103,   300] loss: 0.306\n",
            "Accuracy on test set after epoch 103: 63.10 %\n",
            "[104,   100] loss: 0.268\n",
            "[104,   200] loss: 0.300\n",
            "[104,   300] loss: 0.328\n",
            "Accuracy on test set after epoch 104: 61.84 %\n",
            "[105,   100] loss: 0.283\n",
            "[105,   200] loss: 0.300\n",
            "[105,   300] loss: 0.308\n",
            "Accuracy on test set after epoch 105: 62.55 %\n",
            "[106,   100] loss: 0.291\n",
            "[106,   200] loss: 0.289\n",
            "[106,   300] loss: 0.310\n",
            "Accuracy on test set after epoch 106: 62.52 %\n",
            "[107,   100] loss: 0.285\n",
            "[107,   200] loss: 0.298\n",
            "[107,   300] loss: 0.313\n",
            "Accuracy on test set after epoch 107: 63.10 %\n",
            "[108,   100] loss: 0.268\n",
            "[108,   200] loss: 0.282\n",
            "[108,   300] loss: 0.300\n",
            "Accuracy on test set after epoch 108: 63.32 %\n",
            "[109,   100] loss: 0.258\n",
            "[109,   200] loss: 0.293\n",
            "[109,   300] loss: 0.291\n",
            "Accuracy on test set after epoch 109: 62.40 %\n",
            "[110,   100] loss: 0.272\n",
            "[110,   200] loss: 0.282\n",
            "[110,   300] loss: 0.301\n",
            "Accuracy on test set after epoch 110: 63.67 %\n",
            "[111,   100] loss: 0.271\n",
            "[111,   200] loss: 0.291\n",
            "[111,   300] loss: 0.302\n",
            "Accuracy on test set after epoch 111: 62.86 %\n",
            "[112,   100] loss: 0.271\n",
            "[112,   200] loss: 0.305\n",
            "[112,   300] loss: 0.295\n",
            "Accuracy on test set after epoch 112: 62.08 %\n",
            "[113,   100] loss: 0.268\n",
            "[113,   200] loss: 0.257\n",
            "[113,   300] loss: 0.304\n",
            "Accuracy on test set after epoch 113: 62.20 %\n",
            "[114,   100] loss: 0.250\n",
            "[114,   200] loss: 0.260\n",
            "[114,   300] loss: 0.294\n",
            "Accuracy on test set after epoch 114: 62.42 %\n",
            "[115,   100] loss: 0.257\n",
            "[115,   200] loss: 0.285\n",
            "[115,   300] loss: 0.293\n",
            "Accuracy on test set after epoch 115: 62.70 %\n",
            "[116,   100] loss: 0.251\n",
            "[116,   200] loss: 0.257\n",
            "[116,   300] loss: 0.294\n",
            "Accuracy on test set after epoch 116: 63.47 %\n",
            "[117,   100] loss: 0.253\n",
            "[117,   200] loss: 0.258\n",
            "[117,   300] loss: 0.276\n",
            "Accuracy on test set after epoch 117: 62.60 %\n",
            "[118,   100] loss: 0.241\n",
            "[118,   200] loss: 0.262\n",
            "[118,   300] loss: 0.277\n",
            "Accuracy on test set after epoch 118: 63.72 %\n",
            "[119,   100] loss: 0.238\n",
            "[119,   200] loss: 0.266\n",
            "[119,   300] loss: 0.275\n",
            "Accuracy on test set after epoch 119: 62.45 %\n",
            "[120,   100] loss: 0.250\n",
            "[120,   200] loss: 0.265\n",
            "[120,   300] loss: 0.261\n",
            "Accuracy on test set after epoch 120: 62.88 %\n",
            "[121,   100] loss: 0.255\n",
            "[121,   200] loss: 0.256\n",
            "[121,   300] loss: 0.265\n",
            "Accuracy on test set after epoch 121: 62.03 %\n",
            "[122,   100] loss: 0.263\n",
            "[122,   200] loss: 0.267\n",
            "[122,   300] loss: 0.250\n",
            "Accuracy on test set after epoch 122: 63.33 %\n",
            "[123,   100] loss: 0.233\n",
            "[123,   200] loss: 0.255\n",
            "[123,   300] loss: 0.280\n",
            "Accuracy on test set after epoch 123: 62.35 %\n",
            "[124,   100] loss: 0.246\n",
            "[124,   200] loss: 0.249\n",
            "[124,   300] loss: 0.272\n",
            "Accuracy on test set after epoch 124: 62.89 %\n",
            "[125,   100] loss: 0.242\n",
            "[125,   200] loss: 0.252\n",
            "[125,   300] loss: 0.263\n",
            "Accuracy on test set after epoch 125: 63.05 %\n",
            "[126,   100] loss: 0.261\n",
            "[126,   200] loss: 0.253\n",
            "[126,   300] loss: 0.267\n",
            "Accuracy on test set after epoch 126: 62.36 %\n",
            "[127,   100] loss: 0.230\n",
            "[127,   200] loss: 0.238\n",
            "[127,   300] loss: 0.253\n",
            "Accuracy on test set after epoch 127: 63.23 %\n",
            "[128,   100] loss: 0.228\n",
            "[128,   200] loss: 0.250\n",
            "[128,   300] loss: 0.272\n",
            "Accuracy on test set after epoch 128: 63.40 %\n",
            "[129,   100] loss: 0.225\n",
            "[129,   200] loss: 0.261\n",
            "[129,   300] loss: 0.248\n",
            "Accuracy on test set after epoch 129: 63.82 %\n",
            "[130,   100] loss: 0.226\n",
            "[130,   200] loss: 0.245\n",
            "[130,   300] loss: 0.276\n",
            "Accuracy on test set after epoch 130: 63.41 %\n",
            "[131,   100] loss: 0.251\n",
            "[131,   200] loss: 0.236\n",
            "[131,   300] loss: 0.249\n",
            "Accuracy on test set after epoch 131: 63.07 %\n",
            "[132,   100] loss: 0.217\n",
            "[132,   200] loss: 0.233\n",
            "[132,   300] loss: 0.246\n",
            "Accuracy on test set after epoch 132: 63.12 %\n",
            "[133,   100] loss: 0.222\n",
            "[133,   200] loss: 0.223\n",
            "[133,   300] loss: 0.250\n",
            "Accuracy on test set after epoch 133: 63.15 %\n",
            "[134,   100] loss: 0.250\n",
            "[134,   200] loss: 0.247\n",
            "[134,   300] loss: 0.258\n",
            "Accuracy on test set after epoch 134: 63.05 %\n",
            "[135,   100] loss: 0.231\n",
            "[135,   200] loss: 0.233\n",
            "[135,   300] loss: 0.243\n",
            "Accuracy on test set after epoch 135: 62.91 %\n",
            "[136,   100] loss: 0.236\n",
            "[136,   200] loss: 0.224\n",
            "[136,   300] loss: 0.256\n",
            "Accuracy on test set after epoch 136: 63.60 %\n",
            "[137,   100] loss: 0.223\n",
            "[137,   200] loss: 0.222\n",
            "[137,   300] loss: 0.258\n",
            "Accuracy on test set after epoch 137: 61.84 %\n",
            "[138,   100] loss: 0.234\n",
            "[138,   200] loss: 0.234\n",
            "[138,   300] loss: 0.246\n",
            "Accuracy on test set after epoch 138: 64.03 %\n",
            "[139,   100] loss: 0.191\n",
            "[139,   200] loss: 0.237\n",
            "[139,   300] loss: 0.238\n",
            "Accuracy on test set after epoch 139: 62.73 %\n",
            "[140,   100] loss: 0.225\n",
            "[140,   200] loss: 0.231\n",
            "[140,   300] loss: 0.227\n",
            "Accuracy on test set after epoch 140: 63.42 %\n",
            "[141,   100] loss: 0.211\n",
            "[141,   200] loss: 0.227\n",
            "[141,   300] loss: 0.238\n",
            "Accuracy on test set after epoch 141: 63.12 %\n",
            "[142,   100] loss: 0.210\n",
            "[142,   200] loss: 0.215\n",
            "[142,   300] loss: 0.234\n",
            "Accuracy on test set after epoch 142: 63.16 %\n",
            "[143,   100] loss: 0.219\n",
            "[143,   200] loss: 0.222\n",
            "[143,   300] loss: 0.247\n",
            "Accuracy on test set after epoch 143: 63.50 %\n",
            "[144,   100] loss: 0.204\n",
            "[144,   200] loss: 0.214\n",
            "[144,   300] loss: 0.229\n",
            "Accuracy on test set after epoch 144: 63.04 %\n",
            "[145,   100] loss: 0.216\n",
            "[145,   200] loss: 0.225\n",
            "[145,   300] loss: 0.231\n",
            "Accuracy on test set after epoch 145: 63.37 %\n",
            "[146,   100] loss: 0.214\n",
            "[146,   200] loss: 0.219\n",
            "[146,   300] loss: 0.228\n",
            "Accuracy on test set after epoch 146: 63.77 %\n",
            "[147,   100] loss: 0.201\n",
            "[147,   200] loss: 0.211\n",
            "[147,   300] loss: 0.223\n",
            "Accuracy on test set after epoch 147: 63.63 %\n",
            "[148,   100] loss: 0.208\n",
            "[148,   200] loss: 0.237\n",
            "[148,   300] loss: 0.236\n",
            "Accuracy on test set after epoch 148: 62.23 %\n",
            "[149,   100] loss: 0.215\n",
            "[149,   200] loss: 0.227\n",
            "[149,   300] loss: 0.247\n",
            "Accuracy on test set after epoch 149: 63.18 %\n",
            "[150,   100] loss: 0.199\n",
            "[150,   200] loss: 0.221\n",
            "[150,   300] loss: 0.231\n",
            "Accuracy on test set after epoch 150: 63.69 %\n",
            "[151,   100] loss: 0.205\n",
            "[151,   200] loss: 0.209\n",
            "[151,   300] loss: 0.232\n",
            "Accuracy on test set after epoch 151: 63.92 %\n",
            "[152,   100] loss: 0.201\n",
            "[152,   200] loss: 0.227\n",
            "[152,   300] loss: 0.235\n",
            "Accuracy on test set after epoch 152: 64.24 %\n",
            "[153,   100] loss: 0.214\n",
            "[153,   200] loss: 0.221\n",
            "[153,   300] loss: 0.209\n",
            "Accuracy on test set after epoch 153: 64.35 %\n",
            "[154,   100] loss: 0.204\n",
            "[154,   200] loss: 0.203\n",
            "[154,   300] loss: 0.230\n",
            "Accuracy on test set after epoch 154: 64.19 %\n",
            "[155,   100] loss: 0.195\n",
            "[155,   200] loss: 0.211\n",
            "[155,   300] loss: 0.228\n",
            "Accuracy on test set after epoch 155: 63.15 %\n",
            "[156,   100] loss: 0.199\n",
            "[156,   200] loss: 0.202\n",
            "[156,   300] loss: 0.222\n",
            "Accuracy on test set after epoch 156: 63.18 %\n",
            "[157,   100] loss: 0.201\n",
            "[157,   200] loss: 0.217\n",
            "[157,   300] loss: 0.226\n",
            "Accuracy on test set after epoch 157: 63.11 %\n",
            "[158,   100] loss: 0.196\n",
            "[158,   200] loss: 0.211\n",
            "[158,   300] loss: 0.221\n",
            "Accuracy on test set after epoch 158: 63.45 %\n",
            "[159,   100] loss: 0.199\n",
            "[159,   200] loss: 0.208\n",
            "[159,   300] loss: 0.212\n",
            "Accuracy on test set after epoch 159: 63.08 %\n",
            "[160,   100] loss: 0.195\n",
            "[160,   200] loss: 0.200\n",
            "[160,   300] loss: 0.217\n",
            "Accuracy on test set after epoch 160: 63.27 %\n",
            "[161,   100] loss: 0.191\n",
            "[161,   200] loss: 0.184\n",
            "[161,   300] loss: 0.210\n",
            "Accuracy on test set after epoch 161: 63.98 %\n",
            "[162,   100] loss: 0.197\n",
            "[162,   200] loss: 0.204\n",
            "[162,   300] loss: 0.228\n",
            "Accuracy on test set after epoch 162: 63.72 %\n",
            "[163,   100] loss: 0.190\n",
            "[163,   200] loss: 0.210\n",
            "[163,   300] loss: 0.212\n",
            "Accuracy on test set after epoch 163: 63.60 %\n",
            "[164,   100] loss: 0.197\n",
            "[164,   200] loss: 0.214\n",
            "[164,   300] loss: 0.219\n",
            "Accuracy on test set after epoch 164: 63.93 %\n",
            "[165,   100] loss: 0.183\n",
            "[165,   200] loss: 0.202\n",
            "[165,   300] loss: 0.213\n",
            "Accuracy on test set after epoch 165: 63.59 %\n",
            "[166,   100] loss: 0.198\n",
            "[166,   200] loss: 0.205\n",
            "[166,   300] loss: 0.220\n",
            "Accuracy on test set after epoch 166: 63.08 %\n",
            "[167,   100] loss: 0.183\n",
            "[167,   200] loss: 0.209\n",
            "[167,   300] loss: 0.220\n",
            "Accuracy on test set after epoch 167: 63.79 %\n",
            "[168,   100] loss: 0.191\n",
            "[168,   200] loss: 0.196\n",
            "[168,   300] loss: 0.205\n",
            "Accuracy on test set after epoch 168: 63.06 %\n",
            "[169,   100] loss: 0.192\n",
            "[169,   200] loss: 0.184\n",
            "[169,   300] loss: 0.209\n",
            "Accuracy on test set after epoch 169: 63.67 %\n",
            "[170,   100] loss: 0.184\n",
            "[170,   200] loss: 0.201\n",
            "[170,   300] loss: 0.208\n",
            "Accuracy on test set after epoch 170: 63.05 %\n",
            "[171,   100] loss: 0.189\n",
            "[171,   200] loss: 0.202\n",
            "[171,   300] loss: 0.215\n",
            "Accuracy on test set after epoch 171: 63.10 %\n",
            "[172,   100] loss: 0.182\n",
            "[172,   200] loss: 0.200\n",
            "[172,   300] loss: 0.212\n",
            "Accuracy on test set after epoch 172: 63.65 %\n",
            "[173,   100] loss: 0.198\n",
            "[173,   200] loss: 0.184\n",
            "[173,   300] loss: 0.222\n",
            "Accuracy on test set after epoch 173: 63.30 %\n",
            "[174,   100] loss: 0.178\n",
            "[174,   200] loss: 0.199\n",
            "[174,   300] loss: 0.207\n",
            "Accuracy on test set after epoch 174: 63.75 %\n",
            "[175,   100] loss: 0.177\n",
            "[175,   200] loss: 0.190\n",
            "[175,   300] loss: 0.203\n",
            "Accuracy on test set after epoch 175: 64.07 %\n",
            "[176,   100] loss: 0.178\n",
            "[176,   200] loss: 0.184\n",
            "[176,   300] loss: 0.217\n",
            "Accuracy on test set after epoch 176: 64.08 %\n",
            "[177,   100] loss: 0.188\n",
            "[177,   200] loss: 0.194\n",
            "[177,   300] loss: 0.196\n",
            "Accuracy on test set after epoch 177: 64.29 %\n",
            "[178,   100] loss: 0.188\n",
            "[178,   200] loss: 0.201\n",
            "[178,   300] loss: 0.198\n",
            "Accuracy on test set after epoch 178: 64.53 %\n",
            "[179,   100] loss: 0.186\n",
            "[179,   200] loss: 0.197\n",
            "[179,   300] loss: 0.187\n",
            "Accuracy on test set after epoch 179: 63.93 %\n",
            "[180,   100] loss: 0.182\n",
            "[180,   200] loss: 0.197\n",
            "[180,   300] loss: 0.190\n",
            "Accuracy on test set after epoch 180: 63.07 %\n",
            "[181,   100] loss: 0.171\n",
            "[181,   200] loss: 0.206\n",
            "[181,   300] loss: 0.202\n",
            "Accuracy on test set after epoch 181: 64.00 %\n",
            "[182,   100] loss: 0.168\n",
            "[182,   200] loss: 0.187\n",
            "[182,   300] loss: 0.201\n",
            "Accuracy on test set after epoch 182: 63.54 %\n",
            "[183,   100] loss: 0.164\n",
            "[183,   200] loss: 0.174\n",
            "[183,   300] loss: 0.198\n",
            "Accuracy on test set after epoch 183: 62.26 %\n",
            "[184,   100] loss: 0.172\n",
            "[184,   200] loss: 0.180\n",
            "[184,   300] loss: 0.189\n",
            "Accuracy on test set after epoch 184: 63.24 %\n",
            "[185,   100] loss: 0.188\n",
            "[185,   200] loss: 0.187\n",
            "[185,   300] loss: 0.196\n",
            "Accuracy on test set after epoch 185: 62.61 %\n",
            "[186,   100] loss: 0.181\n",
            "[186,   200] loss: 0.195\n",
            "[186,   300] loss: 0.198\n",
            "Accuracy on test set after epoch 186: 62.98 %\n",
            "[187,   100] loss: 0.167\n",
            "[187,   200] loss: 0.168\n",
            "[187,   300] loss: 0.186\n",
            "Accuracy on test set after epoch 187: 63.81 %\n",
            "[188,   100] loss: 0.177\n",
            "[188,   200] loss: 0.195\n",
            "[188,   300] loss: 0.187\n",
            "Accuracy on test set after epoch 188: 62.98 %\n",
            "[189,   100] loss: 0.181\n",
            "[189,   200] loss: 0.180\n",
            "[189,   300] loss: 0.187\n",
            "Accuracy on test set after epoch 189: 63.60 %\n",
            "[190,   100] loss: 0.169\n",
            "[190,   200] loss: 0.180\n",
            "[190,   300] loss: 0.205\n",
            "Accuracy on test set after epoch 190: 63.78 %\n",
            "[191,   100] loss: 0.173\n",
            "[191,   200] loss: 0.177\n",
            "[191,   300] loss: 0.198\n",
            "Accuracy on test set after epoch 191: 63.70 %\n",
            "[192,   100] loss: 0.171\n",
            "[192,   200] loss: 0.174\n",
            "[192,   300] loss: 0.193\n",
            "Accuracy on test set after epoch 192: 63.47 %\n",
            "[193,   100] loss: 0.174\n",
            "[193,   200] loss: 0.188\n",
            "[193,   300] loss: 0.189\n",
            "Accuracy on test set after epoch 193: 63.70 %\n",
            "[194,   100] loss: 0.176\n",
            "[194,   200] loss: 0.177\n",
            "[194,   300] loss: 0.189\n",
            "Accuracy on test set after epoch 194: 63.10 %\n",
            "[195,   100] loss: 0.168\n",
            "[195,   200] loss: 0.182\n",
            "[195,   300] loss: 0.193\n",
            "Accuracy on test set after epoch 195: 64.06 %\n",
            "[196,   100] loss: 0.173\n",
            "[196,   200] loss: 0.173\n",
            "[196,   300] loss: 0.184\n",
            "Accuracy on test set after epoch 196: 63.56 %\n",
            "[197,   100] loss: 0.174\n",
            "[197,   200] loss: 0.196\n",
            "[197,   300] loss: 0.196\n",
            "Accuracy on test set after epoch 197: 63.46 %\n",
            "[198,   100] loss: 0.165\n",
            "[198,   200] loss: 0.183\n",
            "[198,   300] loss: 0.199\n",
            "Accuracy on test set after epoch 198: 64.21 %\n",
            "[199,   100] loss: 0.165\n",
            "[199,   200] loss: 0.168\n",
            "[199,   300] loss: 0.177\n",
            "Accuracy on test set after epoch 199: 63.79 %\n",
            "[200,   100] loss: 0.150\n",
            "[200,   200] loss: 0.162\n",
            "[200,   300] loss: 0.173\n",
            "Accuracy on test set after epoch 200: 63.59 %\n",
            "[201,   100] loss: 0.168\n",
            "[201,   200] loss: 0.183\n",
            "[201,   300] loss: 0.187\n",
            "Accuracy on test set after epoch 201: 63.70 %\n",
            "[202,   100] loss: 0.156\n",
            "[202,   200] loss: 0.170\n",
            "[202,   300] loss: 0.189\n",
            "Accuracy on test set after epoch 202: 63.44 %\n",
            "[203,   100] loss: 0.175\n",
            "[203,   200] loss: 0.191\n",
            "[203,   300] loss: 0.182\n",
            "Accuracy on test set after epoch 203: 63.68 %\n",
            "[204,   100] loss: 0.171\n",
            "[204,   200] loss: 0.175\n",
            "[204,   300] loss: 0.189\n",
            "Accuracy on test set after epoch 204: 63.56 %\n",
            "[205,   100] loss: 0.170\n",
            "[205,   200] loss: 0.172\n",
            "[205,   300] loss: 0.186\n",
            "Accuracy on test set after epoch 205: 63.66 %\n",
            "[206,   100] loss: 0.161\n",
            "[206,   200] loss: 0.171\n",
            "[206,   300] loss: 0.175\n",
            "Accuracy on test set after epoch 206: 63.73 %\n",
            "[207,   100] loss: 0.163\n",
            "[207,   200] loss: 0.172\n",
            "[207,   300] loss: 0.174\n",
            "Accuracy on test set after epoch 207: 63.98 %\n",
            "[208,   100] loss: 0.158\n",
            "[208,   200] loss: 0.169\n",
            "[208,   300] loss: 0.170\n",
            "Accuracy on test set after epoch 208: 63.90 %\n",
            "[209,   100] loss: 0.161\n",
            "[209,   200] loss: 0.151\n",
            "[209,   300] loss: 0.183\n",
            "Accuracy on test set after epoch 209: 63.22 %\n",
            "[210,   100] loss: 0.175\n",
            "[210,   200] loss: 0.165\n",
            "[210,   300] loss: 0.181\n",
            "Accuracy on test set after epoch 210: 62.90 %\n",
            "[211,   100] loss: 0.183\n",
            "[211,   200] loss: 0.169\n",
            "[211,   300] loss: 0.181\n",
            "Accuracy on test set after epoch 211: 63.35 %\n",
            "[212,   100] loss: 0.166\n",
            "[212,   200] loss: 0.169\n",
            "[212,   300] loss: 0.184\n",
            "Accuracy on test set after epoch 212: 63.09 %\n",
            "[213,   100] loss: 0.171\n",
            "[213,   200] loss: 0.179\n",
            "[213,   300] loss: 0.190\n",
            "Accuracy on test set after epoch 213: 64.17 %\n",
            "[214,   100] loss: 0.168\n",
            "[214,   200] loss: 0.164\n",
            "[214,   300] loss: 0.179\n",
            "Accuracy on test set after epoch 214: 64.50 %\n",
            "[215,   100] loss: 0.158\n",
            "[215,   200] loss: 0.157\n",
            "[215,   300] loss: 0.172\n",
            "Accuracy on test set after epoch 215: 63.84 %\n",
            "[216,   100] loss: 0.158\n",
            "[216,   200] loss: 0.169\n",
            "[216,   300] loss: 0.175\n",
            "Accuracy on test set after epoch 216: 62.97 %\n",
            "[217,   100] loss: 0.164\n",
            "[217,   200] loss: 0.159\n",
            "[217,   300] loss: 0.176\n",
            "Accuracy on test set after epoch 217: 64.11 %\n",
            "[218,   100] loss: 0.157\n",
            "[218,   200] loss: 0.178\n",
            "[218,   300] loss: 0.180\n",
            "Accuracy on test set after epoch 218: 63.73 %\n",
            "[219,   100] loss: 0.155\n",
            "[219,   200] loss: 0.155\n",
            "[219,   300] loss: 0.159\n",
            "Accuracy on test set after epoch 219: 63.30 %\n",
            "[220,   100] loss: 0.155\n",
            "[220,   200] loss: 0.163\n",
            "[220,   300] loss: 0.172\n",
            "Accuracy on test set after epoch 220: 63.67 %\n",
            "[221,   100] loss: 0.165\n",
            "[221,   200] loss: 0.165\n",
            "[221,   300] loss: 0.175\n",
            "Accuracy on test set after epoch 221: 63.69 %\n",
            "[222,   100] loss: 0.163\n",
            "[222,   200] loss: 0.176\n",
            "[222,   300] loss: 0.173\n",
            "Accuracy on test set after epoch 222: 63.81 %\n",
            "[223,   100] loss: 0.159\n",
            "[223,   200] loss: 0.150\n",
            "[223,   300] loss: 0.170\n",
            "Accuracy on test set after epoch 223: 63.95 %\n",
            "[224,   100] loss: 0.154\n",
            "[224,   200] loss: 0.162\n",
            "[224,   300] loss: 0.179\n",
            "Accuracy on test set after epoch 224: 64.01 %\n",
            "[225,   100] loss: 0.174\n",
            "[225,   200] loss: 0.169\n",
            "[225,   300] loss: 0.175\n",
            "Accuracy on test set after epoch 225: 64.39 %\n",
            "[226,   100] loss: 0.160\n",
            "[226,   200] loss: 0.157\n",
            "[226,   300] loss: 0.167\n",
            "Accuracy on test set after epoch 226: 62.83 %\n",
            "[227,   100] loss: 0.161\n",
            "[227,   200] loss: 0.170\n",
            "[227,   300] loss: 0.165\n",
            "Accuracy on test set after epoch 227: 64.13 %\n",
            "[228,   100] loss: 0.159\n",
            "[228,   200] loss: 0.172\n",
            "[228,   300] loss: 0.168\n",
            "Accuracy on test set after epoch 228: 64.40 %\n",
            "[229,   100] loss: 0.147\n",
            "[229,   200] loss: 0.159\n",
            "[229,   300] loss: 0.170\n",
            "Accuracy on test set after epoch 229: 64.12 %\n",
            "[230,   100] loss: 0.150\n",
            "[230,   200] loss: 0.154\n",
            "[230,   300] loss: 0.163\n",
            "Accuracy on test set after epoch 230: 64.56 %\n",
            "[231,   100] loss: 0.164\n",
            "[231,   200] loss: 0.163\n",
            "[231,   300] loss: 0.174\n",
            "Accuracy on test set after epoch 231: 63.71 %\n",
            "[232,   100] loss: 0.151\n",
            "[232,   200] loss: 0.153\n",
            "[232,   300] loss: 0.163\n",
            "Accuracy on test set after epoch 232: 63.99 %\n",
            "[233,   100] loss: 0.161\n",
            "[233,   200] loss: 0.160\n",
            "[233,   300] loss: 0.178\n",
            "Accuracy on test set after epoch 233: 63.56 %\n",
            "[234,   100] loss: 0.148\n",
            "[234,   200] loss: 0.164\n",
            "[234,   300] loss: 0.162\n",
            "Accuracy on test set after epoch 234: 63.14 %\n",
            "[235,   100] loss: 0.156\n",
            "[235,   200] loss: 0.167\n",
            "[235,   300] loss: 0.156\n",
            "Accuracy on test set after epoch 235: 64.18 %\n",
            "[236,   100] loss: 0.143\n",
            "[236,   200] loss: 0.148\n",
            "[236,   300] loss: 0.169\n",
            "Accuracy on test set after epoch 236: 63.24 %\n",
            "[237,   100] loss: 0.145\n",
            "[237,   200] loss: 0.158\n",
            "[237,   300] loss: 0.173\n",
            "Accuracy on test set after epoch 237: 63.77 %\n",
            "[238,   100] loss: 0.150\n",
            "[238,   200] loss: 0.168\n",
            "[238,   300] loss: 0.156\n",
            "Accuracy on test set after epoch 238: 62.91 %\n",
            "[239,   100] loss: 0.133\n",
            "[239,   200] loss: 0.150\n",
            "[239,   300] loss: 0.174\n",
            "Accuracy on test set after epoch 239: 63.50 %\n",
            "[240,   100] loss: 0.158\n",
            "[240,   200] loss: 0.159\n",
            "[240,   300] loss: 0.156\n",
            "Accuracy on test set after epoch 240: 64.29 %\n",
            "[241,   100] loss: 0.153\n",
            "[241,   200] loss: 0.162\n",
            "[241,   300] loss: 0.165\n",
            "Accuracy on test set after epoch 241: 63.63 %\n",
            "[242,   100] loss: 0.151\n",
            "[242,   200] loss: 0.149\n",
            "[242,   300] loss: 0.177\n",
            "Accuracy on test set after epoch 242: 63.64 %\n",
            "[243,   100] loss: 0.140\n",
            "[243,   200] loss: 0.142\n",
            "[243,   300] loss: 0.158\n",
            "Accuracy on test set after epoch 243: 63.27 %\n",
            "[244,   100] loss: 0.159\n",
            "[244,   200] loss: 0.162\n",
            "[244,   300] loss: 0.181\n",
            "Accuracy on test set after epoch 244: 64.03 %\n",
            "[245,   100] loss: 0.151\n",
            "[245,   200] loss: 0.150\n",
            "[245,   300] loss: 0.168\n",
            "Accuracy on test set after epoch 245: 64.15 %\n",
            "[246,   100] loss: 0.146\n",
            "[246,   200] loss: 0.151\n",
            "[246,   300] loss: 0.149\n",
            "Accuracy on test set after epoch 246: 64.77 %\n",
            "[247,   100] loss: 0.136\n",
            "[247,   200] loss: 0.149\n",
            "[247,   300] loss: 0.168\n",
            "Accuracy on test set after epoch 247: 62.77 %\n",
            "[248,   100] loss: 0.153\n",
            "[248,   200] loss: 0.148\n",
            "[248,   300] loss: 0.163\n",
            "Accuracy on test set after epoch 248: 63.66 %\n",
            "[249,   100] loss: 0.144\n",
            "[249,   200] loss: 0.164\n",
            "[249,   300] loss: 0.173\n",
            "Accuracy on test set after epoch 249: 63.39 %\n",
            "[250,   100] loss: 0.156\n",
            "[250,   200] loss: 0.166\n",
            "[250,   300] loss: 0.165\n",
            "Accuracy on test set after epoch 250: 64.22 %\n",
            "[251,   100] loss: 0.142\n",
            "[251,   200] loss: 0.144\n",
            "[251,   300] loss: 0.162\n",
            "Accuracy on test set after epoch 251: 64.40 %\n",
            "[252,   100] loss: 0.140\n",
            "[252,   200] loss: 0.148\n",
            "[252,   300] loss: 0.160\n",
            "Accuracy on test set after epoch 252: 64.09 %\n",
            "[253,   100] loss: 0.141\n",
            "[253,   200] loss: 0.154\n",
            "[253,   300] loss: 0.170\n",
            "Accuracy on test set after epoch 253: 62.63 %\n",
            "[254,   100] loss: 0.150\n",
            "[254,   200] loss: 0.151\n",
            "[254,   300] loss: 0.167\n",
            "Accuracy on test set after epoch 254: 63.84 %\n",
            "[255,   100] loss: 0.172\n",
            "[255,   200] loss: 0.170\n",
            "[255,   300] loss: 0.158\n",
            "Accuracy on test set after epoch 255: 63.52 %\n",
            "[256,   100] loss: 0.136\n",
            "[256,   200] loss: 0.138\n",
            "[256,   300] loss: 0.142\n",
            "Accuracy on test set after epoch 256: 64.12 %\n",
            "[257,   100] loss: 0.142\n",
            "[257,   200] loss: 0.146\n",
            "[257,   300] loss: 0.166\n",
            "Accuracy on test set after epoch 257: 63.95 %\n",
            "[258,   100] loss: 0.155\n",
            "[258,   200] loss: 0.164\n",
            "[258,   300] loss: 0.160\n",
            "Accuracy on test set after epoch 258: 63.62 %\n",
            "[259,   100] loss: 0.148\n",
            "[259,   200] loss: 0.155\n",
            "[259,   300] loss: 0.163\n",
            "Accuracy on test set after epoch 259: 63.77 %\n",
            "[260,   100] loss: 0.142\n",
            "[260,   200] loss: 0.137\n",
            "[260,   300] loss: 0.148\n",
            "Accuracy on test set after epoch 260: 63.79 %\n",
            "[261,   100] loss: 0.132\n",
            "[261,   200] loss: 0.147\n",
            "[261,   300] loss: 0.164\n",
            "Accuracy on test set after epoch 261: 63.56 %\n",
            "[262,   100] loss: 0.152\n",
            "[262,   200] loss: 0.155\n",
            "[262,   300] loss: 0.153\n",
            "Accuracy on test set after epoch 262: 63.05 %\n",
            "[263,   100] loss: 0.145\n",
            "[263,   200] loss: 0.148\n",
            "[263,   300] loss: 0.165\n",
            "Accuracy on test set after epoch 263: 63.87 %\n",
            "[264,   100] loss: 0.139\n",
            "[264,   200] loss: 0.153\n",
            "[264,   300] loss: 0.166\n",
            "Accuracy on test set after epoch 264: 63.70 %\n",
            "[265,   100] loss: 0.145\n",
            "[265,   200] loss: 0.150\n",
            "[265,   300] loss: 0.152\n",
            "Accuracy on test set after epoch 265: 63.47 %\n",
            "[266,   100] loss: 0.133\n",
            "[266,   200] loss: 0.150\n",
            "[266,   300] loss: 0.173\n",
            "Accuracy on test set after epoch 266: 63.98 %\n",
            "[267,   100] loss: 0.131\n",
            "[267,   200] loss: 0.158\n",
            "[267,   300] loss: 0.155\n",
            "Accuracy on test set after epoch 267: 64.02 %\n",
            "[268,   100] loss: 0.134\n",
            "[268,   200] loss: 0.145\n",
            "[268,   300] loss: 0.154\n",
            "Accuracy on test set after epoch 268: 63.88 %\n",
            "[269,   100] loss: 0.159\n",
            "[269,   200] loss: 0.159\n",
            "[269,   300] loss: 0.163\n",
            "Accuracy on test set after epoch 269: 64.10 %\n",
            "[270,   100] loss: 0.129\n",
            "[270,   200] loss: 0.150\n",
            "[270,   300] loss: 0.158\n",
            "Accuracy on test set after epoch 270: 63.26 %\n",
            "[271,   100] loss: 0.132\n",
            "[271,   200] loss: 0.142\n",
            "[271,   300] loss: 0.157\n",
            "Accuracy on test set after epoch 271: 63.86 %\n",
            "[272,   100] loss: 0.132\n",
            "[272,   200] loss: 0.143\n",
            "[272,   300] loss: 0.156\n",
            "Accuracy on test set after epoch 272: 64.50 %\n",
            "[273,   100] loss: 0.140\n",
            "[273,   200] loss: 0.154\n",
            "[273,   300] loss: 0.161\n",
            "Accuracy on test set after epoch 273: 64.21 %\n",
            "[274,   100] loss: 0.148\n",
            "[274,   200] loss: 0.158\n",
            "[274,   300] loss: 0.159\n",
            "Accuracy on test set after epoch 274: 63.27 %\n",
            "[275,   100] loss: 0.152\n",
            "[275,   200] loss: 0.140\n",
            "[275,   300] loss: 0.147\n",
            "Accuracy on test set after epoch 275: 63.57 %\n",
            "[276,   100] loss: 0.133\n",
            "[276,   200] loss: 0.138\n",
            "[276,   300] loss: 0.153\n",
            "Accuracy on test set after epoch 276: 63.93 %\n",
            "[277,   100] loss: 0.144\n",
            "[277,   200] loss: 0.152\n",
            "[277,   300] loss: 0.155\n",
            "Accuracy on test set after epoch 277: 64.85 %\n",
            "[278,   100] loss: 0.129\n",
            "[278,   200] loss: 0.139\n",
            "[278,   300] loss: 0.146\n",
            "Accuracy on test set after epoch 278: 64.53 %\n",
            "[279,   100] loss: 0.133\n",
            "[279,   200] loss: 0.148\n",
            "[279,   300] loss: 0.152\n",
            "Accuracy on test set after epoch 279: 63.65 %\n",
            "[280,   100] loss: 0.141\n",
            "[280,   200] loss: 0.146\n",
            "[280,   300] loss: 0.148\n",
            "Accuracy on test set after epoch 280: 63.61 %\n",
            "[281,   100] loss: 0.132\n",
            "[281,   200] loss: 0.164\n",
            "[281,   300] loss: 0.164\n",
            "Accuracy on test set after epoch 281: 64.05 %\n",
            "[282,   100] loss: 0.136\n",
            "[282,   200] loss: 0.139\n",
            "[282,   300] loss: 0.147\n",
            "Accuracy on test set after epoch 282: 64.41 %\n",
            "[283,   100] loss: 0.121\n",
            "[283,   200] loss: 0.137\n",
            "[283,   300] loss: 0.153\n",
            "Accuracy on test set after epoch 283: 64.32 %\n",
            "[284,   100] loss: 0.132\n",
            "[284,   200] loss: 0.141\n",
            "[284,   300] loss: 0.149\n",
            "Accuracy on test set after epoch 284: 63.02 %\n",
            "[285,   100] loss: 0.152\n",
            "[285,   200] loss: 0.151\n",
            "[285,   300] loss: 0.153\n",
            "Accuracy on test set after epoch 285: 63.82 %\n",
            "[286,   100] loss: 0.140\n",
            "[286,   200] loss: 0.146\n",
            "[286,   300] loss: 0.153\n",
            "Accuracy on test set after epoch 286: 64.22 %\n",
            "[287,   100] loss: 0.130\n",
            "[287,   200] loss: 0.141\n",
            "[287,   300] loss: 0.168\n",
            "Accuracy on test set after epoch 287: 63.53 %\n",
            "[288,   100] loss: 0.136\n",
            "[288,   200] loss: 0.138\n",
            "[288,   300] loss: 0.151\n",
            "Accuracy on test set after epoch 288: 64.75 %\n",
            "[289,   100] loss: 0.129\n",
            "[289,   200] loss: 0.128\n",
            "[289,   300] loss: 0.140\n",
            "Accuracy on test set after epoch 289: 64.17 %\n",
            "[290,   100] loss: 0.138\n",
            "[290,   200] loss: 0.141\n",
            "[290,   300] loss: 0.151\n",
            "Accuracy on test set after epoch 290: 63.35 %\n",
            "[291,   100] loss: 0.143\n",
            "[291,   200] loss: 0.143\n",
            "[291,   300] loss: 0.162\n",
            "Accuracy on test set after epoch 291: 63.46 %\n",
            "[292,   100] loss: 0.140\n",
            "[292,   200] loss: 0.138\n",
            "[292,   300] loss: 0.160\n",
            "Accuracy on test set after epoch 292: 64.27 %\n",
            "[293,   100] loss: 0.130\n",
            "[293,   200] loss: 0.134\n",
            "[293,   300] loss: 0.150\n",
            "Accuracy on test set after epoch 293: 63.34 %\n",
            "[294,   100] loss: 0.132\n",
            "[294,   200] loss: 0.139\n",
            "[294,   300] loss: 0.141\n",
            "Accuracy on test set after epoch 294: 63.31 %\n",
            "[295,   100] loss: 0.143\n",
            "[295,   200] loss: 0.142\n",
            "[295,   300] loss: 0.151\n",
            "Accuracy on test set after epoch 295: 62.81 %\n",
            "[296,   100] loss: 0.141\n",
            "[296,   200] loss: 0.138\n",
            "[296,   300] loss: 0.142\n",
            "Accuracy on test set after epoch 296: 62.52 %\n",
            "[297,   100] loss: 0.139\n",
            "[297,   200] loss: 0.129\n",
            "[297,   300] loss: 0.136\n",
            "Accuracy on test set after epoch 297: 64.66 %\n",
            "[298,   100] loss: 0.134\n",
            "[298,   200] loss: 0.152\n",
            "[298,   300] loss: 0.157\n",
            "Accuracy on test set after epoch 298: 63.92 %\n",
            "[299,   100] loss: 0.128\n",
            "[299,   200] loss: 0.139\n",
            "[299,   300] loss: 0.146\n",
            "Accuracy on test set after epoch 299: 63.08 %\n",
            "[300,   100] loss: 0.124\n",
            "[300,   200] loss: 0.137\n",
            "[300,   300] loss: 0.140\n",
            "Accuracy on test set after epoch 300: 63.49 %\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save trained ResNet-56 weights:"
      ],
      "metadata": {
        "id": "vClLBAJtuUmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_resnet56.state_dict(), '/content/drive/MyDrive/719_project/trained_weights/resnet_model_weights.pth')"
      ],
      "metadata": {
        "id": "-gahXtc9dc0l",
        "outputId": "6e42b597-42da-494b-d903-9bdbfd9b6b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9dbe6edb2882>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_resnet56\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/719_project/trained_weights/resnet_model_weights.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Top-1 accuracy for trained ResNet-56 model and cifar-100 dataset (in paper, 70.43%):"
      ],
      "metadata": {
        "id": "fjgi4ZDmuXFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # for making sure there is no training, just inference\n",
        "    model_resnet56.eval()  # model to eval. mode\n",
        "    for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model_resnet56(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "top1_accuracy = 100 * correct / total\n",
        "print('Top-1 Accuracy: {:.2f}%'.format(top1_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xra3b6yxsrkU",
        "outputId": "f1f8a1f4-ee56-4462-8f09-b89406757ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 Accuracy: 41.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vision Transformer w/ Feature Guidance:**"
      ],
      "metadata": {
        "id": "x_f-1YhVwFmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vision Transformer w/ feature guidance:"
      ],
      "metadata": {
        "id": "Lyc2MRLQwzO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class T2TViT(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, num_classes, embed_dim, depth, heads, mlp_dim, token_dim):\n",
        "        super(T2TViT, self).__init__()\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "        self.depth = depth\n",
        "        self.heads = heads\n",
        "        self.mlp_dim = mlp_dim\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.num_tokens = self.num_patches\n",
        "        self.token_dim = token_dim\n",
        "\n",
        "        self.patch_embeddings = nn.Conv2d(in_channels, self.token_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.token_embeddings = nn.Parameter(torch.randn(1, self.num_tokens, self.token_dim))\n",
        "\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(embed_dim, heads, mlp_dim),\n",
        "            depth\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch embeddings\n",
        "        x = self.patch_embeddings(x)\n",
        "\n",
        "        # Reshaping the patches\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "        # Token embeddings\n",
        "        tokens = self.token_embeddings.repeat(x.shape[0], 1, 1)\n",
        "\n",
        "        # Concatenate token embeddings with patch embeddings\n",
        "        x = torch.cat((tokens, x), dim=1)  # Concatenate along the second dimension\n",
        "\n",
        "        # Transformer layers\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Global pooling (mean)\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        # Classification\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "oLqg-Vm8wGOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Vision Transformer model\n",
        "class VisionTransformer_fg(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim):\n",
        "        super(VisionTransformer_fg, self).__init__()\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = 3 * patch_size ** 2  # Assuming RGB images\n",
        "\n",
        "        self.patch_embedding = nn.Sequential(\n",
        "            nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size),\n",
        "            nn.Flatten(start_dim=2)\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(dim, heads, dim_feedforward=mlp_dim),\n",
        "            num_layers=depth\n",
        "        )\n",
        "        self.classifier = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=0)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YX26rgWHwHo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set training configuration for Vision Transformer w/ feature guidance"
      ],
      "metadata": {
        "id": "QecnTTJPwges"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up hyperparameters and data loaders\n",
        "torch.cuda.empty_cache()\n",
        "image_size = 32\n",
        "patch_size = 16\n",
        "num_classes = 100\n",
        "dim = 128 #64\n",
        "depth = 7 #10\n",
        "heads = 4\n",
        "token_dim = 128\n",
        "mlp_dim = 256 #512\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "epochs = 50\n",
        "BETA = 2.5 # scaler for feature guidance loss\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "\n",
        "# Create an instance of the model\n",
        "#model_ViT = VisionTransformer_fg(image_size, patch_size, num_classes, dim, depth, heads, mlp_dim)\n",
        "model_ViT = T2TViT(image_size, patch_size, 3, num_classes, dim, depth, heads, mlp_dim, token_dim)\n",
        "\n",
        "model_ViT.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_ViT.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "# Create data loaders (replace with your own datasets)\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qPmWYBzwl6x",
        "outputId": "0234145a-9626-4d35-dbbd-7fdb7a4d7521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If pre-trained weights will be used, run following code snippet:"
      ],
      "metadata": {
        "id": "f3mUQRs2y0V-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ViT.load_state_dict(torch.load('/content/drive/MyDrive/719_project/trained_weights/ViT_model_weights.pth'))"
      ],
      "metadata": {
        "id": "HfN4ByVJy4d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vision Transformer Trainer:"
      ],
      "metadata": {
        "id": "q5tBSOANwJum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "#model_resnet56.eval() # no training for cnn, just eval.\n",
        "model_ViT.train()\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    for images, labels in train_dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # zero grads\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs = model_ViT(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_train_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o86la3xZ-SZS",
        "outputId": "0f89edd9-52cc-489f-dc14-fc1c843bf8df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 - Loss: 4.3495\n",
            "Epoch 2/50 - Loss: 4.2698\n",
            "Epoch 3/50 - Loss: 4.2106\n",
            "Epoch 4/50 - Loss: 4.1531\n",
            "Epoch 5/50 - Loss: 4.1188\n",
            "Epoch 6/50 - Loss: 4.0963\n",
            "Epoch 7/50 - Loss: 4.0849\n",
            "Epoch 8/50 - Loss: 4.0796\n",
            "Epoch 9/50 - Loss: 4.0713\n",
            "Epoch 10/50 - Loss: 4.0649\n",
            "Epoch 11/50 - Loss: 4.0593\n",
            "Epoch 12/50 - Loss: 4.0578\n",
            "Epoch 13/50 - Loss: 4.0503\n",
            "Epoch 14/50 - Loss: 4.0532\n",
            "Epoch 15/50 - Loss: 4.0450\n",
            "Epoch 16/50 - Loss: 4.0472\n",
            "Epoch 17/50 - Loss: 4.0429\n",
            "Epoch 18/50 - Loss: 4.0441\n",
            "Epoch 19/50 - Loss: 4.0366\n",
            "Epoch 20/50 - Loss: 4.0358\n",
            "Epoch 21/50 - Loss: 4.0352\n",
            "Epoch 22/50 - Loss: 4.0337\n",
            "Epoch 23/50 - Loss: 4.0305\n",
            "Epoch 24/50 - Loss: 4.0273\n",
            "Epoch 25/50 - Loss: 4.0277\n",
            "Epoch 26/50 - Loss: 4.0224\n",
            "Epoch 27/50 - Loss: 4.0266\n",
            "Epoch 28/50 - Loss: 4.0211\n",
            "Epoch 29/50 - Loss: 4.0210\n",
            "Epoch 30/50 - Loss: 4.0200\n",
            "Epoch 31/50 - Loss: 4.0184\n",
            "Epoch 32/50 - Loss: 4.0156\n",
            "Epoch 33/50 - Loss: 4.0162\n",
            "Epoch 34/50 - Loss: 4.0186\n",
            "Epoch 35/50 - Loss: 4.0146\n",
            "Epoch 36/50 - Loss: 4.0120\n",
            "Epoch 37/50 - Loss: 4.0116\n",
            "Epoch 38/50 - Loss: 4.0060\n",
            "Epoch 39/50 - Loss: 4.0086\n",
            "Epoch 40/50 - Loss: 4.0080\n",
            "Epoch 41/50 - Loss: 4.0021\n",
            "Epoch 42/50 - Loss: 4.0051\n",
            "Epoch 43/50 - Loss: 4.0007\n",
            "Epoch 44/50 - Loss: 4.0012\n",
            "Epoch 45/50 - Loss: 4.0005\n",
            "Epoch 46/50 - Loss: 4.0008\n",
            "Epoch 47/50 - Loss: 4.0006\n",
            "Epoch 48/50 - Loss: 3.9984\n",
            "Epoch 49/50 - Loss: 4.0011\n",
            "Epoch 50/50 - Loss: 3.9978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Top-1 Accuracy for Vision Transformer w/ feature guidance:"
      ],
      "metadata": {
        "id": "Vs5z4OQCKIU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_ViT.state_dict(), '/content/drive/MyDrive/719_project/trained_weights/ViT_model_weights.pth')"
      ],
      "metadata": {
        "id": "At_C5tkjrna9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # for making sure there is no training, just inference\n",
        "    model_ViT.eval()  # model to eval. mode\n",
        "    total, correct = 0, 0\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model_ViT(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "top1_accuracy = 100 * correct / total\n",
        "print('Top-1 Accuracy: {:.2f}%'.format(top1_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K79DxOQaKChG",
        "outputId": "b7bfa789-3dfa-4287-82a8-c1685d97e203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-1 Accuracy: 7.43%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEMP:"
      ],
      "metadata": {
        "id": "f00AkMq0_mNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training loop\n",
        "def train_ViT_with_fg(model_ViT, model_cnn, dataloader, criterion, optimizer, device, BETA):\n",
        "\n",
        "    ###def get_features_hook(module, input, output):\n",
        "      #### Store the intermediate features in a global variable\n",
        "      ###global student_features\n",
        "      ###student_features = output\n",
        "    ###def get_teacher_features_hook(module, input, output):\n",
        "        #### Store the intermediate features in a global variable\n",
        "        ###global teacher_features\n",
        "        ###teacher_features = output\n",
        "\n",
        "    model_cnn.eval() # no training for cnn, just eval.\n",
        "    model_ViT.train()\n",
        "    total_loss = 0.0\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        #images, labels = images.cuda(), labels.cuda() # add this line\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_ViT(images)\n",
        "\n",
        "        #### hook cnn and ViT for intermediate feature extraction #\n",
        "        ###criterion_fg = nn.MSELoss()\n",
        "        ###model_ViT.register_forward_hook(get_features_hook)\n",
        "        #### Register a forward hook to extract features from the teacher model\n",
        "        ###model_cnn.register_forward_hook(get_teacher_features_hook)\n",
        "        ####loss_fg = criterion_fg(student_features, teacher_features.detach())  # detach the teacher features to prevent backpropagation through the teacher\n",
        "        ###loss_fg = 0\n",
        "        # hook cnn and ViT for intermediate feature extraction #\n",
        "        loss_cls = criterion(outputs, labels) # cross-entropy loss\n",
        "        loss = loss_cls + BETA * 0 # loss_fg\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "    return total_loss / len(dataloader.dataset)"
      ],
      "metadata": {
        "id": "OYTQvwCu_puJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}